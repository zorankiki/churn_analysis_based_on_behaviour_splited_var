{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import important_variables\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#### Helpers ####\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "from helpers.s3_bucket_utils import S3BucketUtils\n",
    "from helpers import db_utils\n",
    "from helpers import settings\n",
    "\n",
    "bucket = S3BucketUtils()\n",
    "################\n",
    "\n",
    "for_interpretation = {'model_started':{'did_something_last_X_months':'continued_vs_never_did', 'did_something_before_and_didnt_last_X_months':'stopped_vs_never_did'},\\\n",
    "                     'model_stopped':{'did_something_before':'stopped_vs_never_did', 'did_something_last_X_months':'continued_vs_stopped'}}\n",
    "\n",
    "### read model names and numbers ###\n",
    "model_names = bucket.\\\n",
    "load_csv_from_s3(file_name = 'churn_analysis_based_on_behaviour/combinations_of_variables_that_are_not_dependent/'+\\\n",
    "'model_names.csv')\n",
    "\n",
    "model_names['model_name'] = model_names['model_name'].map(lambda x: \\\n",
    "    list(map(lambda x: x.lstrip(' '), x.replace(\"'\", \"\").split(','))))\n",
    "model_names = model_names.explode('model_name')\n",
    "\n",
    "def add_month(date, m):\n",
    "    ddd = pd.to_datetime(date, format='%Y-%m-%d')\n",
    "    ddd2 = ddd + relativedelta(months=m)\n",
    "    return (str(ddd2))[0:10]\n",
    "\n",
    "def get_key_based_on_value_in_a_dict(dict_, value_of_interest):\n",
    "    for key, value in dict_.items():\n",
    "        if value == value_of_interest:\n",
    "            return key\n",
    "\n",
    "def get_var_type(var_name, first_var, second_var):\n",
    "    if var_name not in first_var.values() and var_name not in second_var.values():\n",
    "        return 'not_behavioural'\n",
    "    elif 'before' in var_name and 'last_month' not in var_name and 'months' not in var_name:\n",
    "        return 'did_something_before'\n",
    "    elif 'before' in var_name and 'didnt' in var_name:\n",
    "        return 'did_something_before_and_didnt_last_X_months'\n",
    "    elif 'before' not in var_name:\n",
    "        return 'did_something_last_X_months'\n",
    "\n",
    "def get_base_var_names(df, first_var, second_var):\n",
    "    df['variable_base_name'] = df['variable']\n",
    "    df['variable_type'] = df['variable'].apply(lambda x: get_var_type(var_name=x, first_var=first_var, second_var=second_var))\n",
    "    for var_ in first_var.values():\n",
    "        if var_ in df['variable'].unique():\n",
    "            df.loc[(df['variable']==var_),  'variable_base_name'] = get_key_based_on_value_in_a_dict(dict_=first_var, \\\n",
    "                                                                                                    value_of_interest=var_)\n",
    "            \n",
    "    for var_ in second_var.values():\n",
    "        if var_ in df['variable'].unique():\n",
    "            df.loc[(df['variable']==var_),  'variable_base_name'] = get_key_based_on_value_in_a_dict(dict_=second_var, \\\n",
    "                                                                                                    value_of_interest=var_)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def check_the_vars_with_the_same_interpretation(x):\n",
    "    if len(x)>1:\n",
    "        if x['interpretation'].iloc[0]==x['interpretation'].iloc[1]:\n",
    "            if x['exp(coef) - AVERAGE'].iloc[0]>1 and x['exp(coef) - AVERAGE'].iloc[1]<1:\n",
    "                return True\n",
    "            elif x['exp(coef) - AVERAGE'].iloc[0]<1 and x['exp(coef) - AVERAGE'].iloc[1]>1: \n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def model_to_choose_from(x):\n",
    "    return x[x['p value - AVERAGE']==min(x['p value - AVERAGE'])]['model'].iloc[0]\n",
    "\n",
    "\n",
    "def get_data_type(df, var_):\n",
    "    if df[var_].nunique()==2 and 1 in df[var_].unique() and 0 in df[var_].unique():\n",
    "        return 'categorical'\n",
    "    else:\n",
    "        return 'numerical'\n",
    "\n",
    "def get_perc_of_spots(df, var_, var_type):\n",
    "    if var_type=='numerical':\n",
    "        return np.nan\n",
    "    elif var_type=='categorical':\n",
    "        return round(100*(df[(df[var_]==1)]['spot_id'].nunique()/df['spot_id'].nunique()), 2)\n",
    "    \n",
    "def get_perc_of_spots_last_month(df, var_, var_type, last_month):\n",
    "    if var_type=='numerical':\n",
    "        return np.nan\n",
    "    elif var_type=='categorical':\n",
    "        return round(100*(df[(df[var_]==1)&\\\n",
    "                            (df['left_limit']==last_month)]['spot_id'].nunique()/df[df['left_limit']==last_month]['spot_id'].nunique()), 2)\n",
    "    \n",
    "    \n",
    "def get_coef_and_p_for_a_specific_model(var_base_name, var_, model, spots_set, model_names, date_dir):\n",
    "    \n",
    "#     var_ had_properly_used_catering_inquiries_last_4_months\n",
    "#     var_base_name c_changed_inquiry_status\n",
    "    if ('changed_inquiry_status' in var_base_name):\n",
    "        with open(r'./parameters/for_properly_used_inquiries_vars.yaml') as file:\n",
    "            for_properly_used_inquiries_vars = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "        var_base_name = for_properly_used_inquiries_vars['changed_inquiry_status_to_properly_used'][var_base_name]\n",
    "\n",
    "    \n",
    "    if var_base_name in model_names['model_name'].unique():\n",
    "        model_number = \\\n",
    "        model_names[(model_names['model_name'].apply(lambda x: var_base_name in x))]['model_number'].values[0]\n",
    "    else:\n",
    "        model_number = 1\n",
    "    if model == 'model_started':\n",
    "        coefs_and_p_values = \\\n",
    "        bucket.load_csv_from_s3(file_name='churn_analysis_based_on_behaviour/data/'+date_dir+'/exports/coefficients_and_pvalues/'+\\\n",
    "                               'started_doing_something/model_'+str(model_number)+'/coef_and_pvalues_'+spots_set+'_p_below_0_2.csv')\n",
    "    else:\n",
    "        coefs_and_p_values = \\\n",
    "        bucket.load_csv_from_s3(file_name='churn_analysis_based_on_behaviour/data/'+date_dir+'/exports/coefficients_and_pvalues/'+\\\n",
    "                               'stopped_doing_something/model_'+str(model_number)+'/coef_and_pvalues_'+spots_set+'_p_below_0_2.csv')\n",
    "    \n",
    "   \n",
    "    if(var_=='had_properly_used_catering_inquiries_last_4_months'):\n",
    "        print('model names', model_names['model_name'].unique())\n",
    "        print('var_', var_)\n",
    "        print('var_base_name', var_base_name)\n",
    "        print('model_number', model_number)\n",
    "        print('spots_set', spots_set)\n",
    "        print(coefs_and_p_values)\n",
    "        \n",
    "    if var_ in coefs_and_p_values['covariate'].unique():\n",
    "        if(var_=='had_properly_used_catering_inquiries_last_4_months'):\n",
    "            print('uso')\n",
    "        return coefs_and_p_values[(coefs_and_p_values['covariate']==var_)][['exp(coef)', 'p']].\\\n",
    "            apply(lambda x: (round(x[0].astype(float), 4), round(x[1].astype(float), 3)), axis = 1).values[0]\n",
    "#         if coefs_and_p_values[(coefs_and_p_values['covariate']==var_)]['p'].values[0]<=0.05:\n",
    "#             return coefs_and_p_values[(coefs_and_p_values['covariate']==var_)][['exp(coef)', 'p']].\\\n",
    "#             apply(lambda x: (round(x[0], 4), round(x[1], 3)), axis = 1).values[0]\n",
    "#         else:\n",
    "#             return np.nan\n",
    "    elif var_ not in coefs_and_p_values['covariate'].unique():\n",
    "        if(var_=='had_properly_used_catering_inquiries_last_4_months'):\n",
    "            print('nan')\n",
    "        return np.nan\n",
    "\n",
    "# def get_coef_and_p_for_a_specific_model_for_non_behavioural_vars(var_base_name, var_, model, spots_set, model_names, date_dir):\n",
    "#     for model_number in model_names['model_number'].unique()[1:]:\n",
    "#         if model == 'model_started':\n",
    "#             coefs_and_p_values = \\\n",
    "#             bucket.load_csv_from_s3(file_name='churn_analysis_based_on_behaviour/data/'+date_dir+'/exports/coefficients_and_pvalues/'+\\\n",
    "#                                    'started_doing_something/model_'+str(model_number)+'/coef_and_pvalues_'+spots_set+'_p_below_0_2.csv')\n",
    "#         else:\n",
    "#             coefs_and_p_values = \\\n",
    "#             bucket.load_csv_from_s3(file_name='churn_analysis_based_on_behaviour/data/'+date_dir+'/exports/coefficients_and_pvalues/'+\\\n",
    "#                                'stopped_doing_something/model_'+str(model_number)+'/coef_and_pvalues_'+spots_set+'_p_below_0_2.csv')\n",
    "#         if var_ in coefs_and_p_values['covariate'].unique():\n",
    "#             if coefs_and_p_values[(coefs_and_p_values['covariate']==var_)]['p'].values[0]<=0.05:\n",
    "#                 model_name = model_names[model_names['model_number']==model_number]['model_name'].values[0]\n",
    "#                 return coefs_and_p_values[(coefs_and_p_values['covariate']==var_)][['exp(coef)', 'p']].\\\n",
    "#                       apply(lambda x: (round(x[0], 4), round(x[1], 3), 'model_for_'+str(model_name)), axis = 1).values[0]\n",
    "#     return np.nan\n",
    "    \n",
    "\n",
    "\n",
    "def main(date_of_analysis):\n",
    "    date_dir = date_of_analysis.replace('-', '_')\n",
    "    last_month = add_month(date_of_analysis, -1)\n",
    "    \n",
    "    for file_name in ['all_significant_variables_sorted_by_p_value.csv',\\\n",
    "                 'not_significant_variables_with_p_below_0_2_sorted_by_p_value.csv']:\n",
    "        combined_export = []\n",
    "        for model_type in ['started_doing_something', 'stopped_doing_something']:\n",
    "            (first_var, second_var) = \\\n",
    "            important_variables.get_pairs_of_variables(churn_based_on_behaviour_dir='churn_analysis_based_on_behaviour/',\\\n",
    "                                                   date_dir=date_dir, model_type=model_type)\n",
    "\n",
    "            df_important_vars = \\\n",
    "            pd.read_csv('data/'+date_dir+'/exports/'+model_type+'/'+file_name)\n",
    "\n",
    "            df_important_vars['model'] = 'model_'+model_type.split('_')[0]\n",
    "\n",
    "            df_important_vars = get_base_var_names(df=df_important_vars, first_var=first_var, second_var=second_var)\n",
    "\n",
    "            combined_export.append(df_important_vars)\n",
    "\n",
    "        combined_export = \\\n",
    "        pd.concat([combined_export[0], combined_export[1]], axis = 0)\n",
    "        combined_export.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        combined_export['interpretation'] = 'not a behavioural variable'\n",
    "        combined_export['interpretation'] = \\\n",
    "        combined_export[['model', 'variable_type']].apply(lambda x: for_interpretation[x['model']][x['variable_type']] if x['variable_type']!='not_behavioural' else x['variable_type'], axis = 1)\n",
    "\n",
    "        combined_export = combined_export.merge(combined_export.groupby(['variable_base_name', 'variable_type', 'interpretation'])[['exp(coef) - AVERAGE', 'interpretation']].\\\n",
    "        apply(lambda x: check_the_vars_with_the_same_interpretation(x)).reset_index().rename(columns = {0:'different_sign'}),\\\n",
    "                          on = ['variable_base_name', 'variable_type', 'interpretation'])\n",
    "\n",
    "        if len(combined_export[(combined_export['different_sign']==True)])>0:\n",
    "            print('THERE ARE VARIABLE THAT REPRESENT THE SAME THING BUT HAVE A DIFFERENT SIGN!!!')\n",
    "        else:\n",
    "            combined_export.drop(combined_export[(combined_export['model']=='model_stopped')&\\\n",
    "                                                (combined_export['variable_type']=='did_something_before')].index, inplace = True)\n",
    "\n",
    "            duplicate_vars = \\\n",
    "            combined_export[combined_export['interpretation']=='not_behavioural'].groupby('variable')['model'].nunique()[combined_export[combined_export['interpretation']=='not_behavioural'].groupby('variable')['model'].nunique()>1].\\\n",
    "            reset_index()['variable'].unique()\n",
    "\n",
    "            combined_export.reset_index(drop = True, inplace = True)\n",
    "\n",
    "            df_duplicate_vars = combined_export[(combined_export['variable'].isin(duplicate_vars))].\\\n",
    "            groupby('variable')[['model', 'p value - AVERAGE']].apply(lambda x: model_to_choose_from(x)).\\\n",
    "            reset_index().rename(columns = {0:'model_to_choose_from'})\n",
    "\n",
    "            combined_export = combined_export.merge(df_duplicate_vars, on = ['variable'], how = 'left')\n",
    "\n",
    "            combined_export.drop(combined_export[(combined_export['variable'].isin(duplicate_vars))&\\\n",
    "                                                (combined_export['model']!=combined_export['model_to_choose_from'])].index, inplace = True)\n",
    "\n",
    "        combined_export = combined_export[['variable_base_name', 'variable_type', 'model', 'variable', 'interpretation', 'exp(coef) - AVERAGE', 'p value - AVERAGE']].\\\n",
    "        sort_values(['p value - AVERAGE'])\n",
    "        combined_export.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        combined_export.loc[(combined_export['interpretation']=='not_behavioural'), 'interpretation'] = np.nan\n",
    "\n",
    "        if not os.path.exists('data/'+date_dir+'/exports/important_variables/'):\n",
    "            os.makedirs('data/'+date_dir+'/exports/important_variables/')\n",
    "        combined_export.\\\n",
    "        to_csv('data/'+date_dir+'/exports/important_variables/'+file_name, index = False)\n",
    "        bucket.store_csv_to_s3(data_frame = combined_export, \\\n",
    "            file_name = file_name, \\\n",
    "            dir = '/churn_analysis_based_on_behaviour/data/'+date_dir+'/exports/important_variables/')\n",
    "\n",
    "        if file_name == 'all_significant_variables_sorted_by_p_value.csv':\n",
    "            sign_vars = combined_export\n",
    "            sign_vars['important_variables_group'] = 'significant_vars'\n",
    "        else:\n",
    "            not_sign_p_below_0_2_vars = combined_export\n",
    "            not_sign_p_below_0_2_vars['important_variables_group'] = 'not_significant_p_below_0_2'\n",
    "            \n",
    "    \n",
    "    behavioural_sign_vars_base_names = sign_vars[sign_vars['interpretation'].notnull()]['variable_base_name'].unique()\n",
    "\n",
    "    important_vars = \\\n",
    "    pd.concat([sign_vars, not_sign_p_below_0_2_vars[(not_sign_p_below_0_2_vars['interpretation'].notnull())&\\\n",
    "                             (not_sign_p_below_0_2_vars['variable_base_name'].\\\n",
    "                             isin(behavioural_sign_vars_base_names))]], axis = 0)\n",
    "    important_vars.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    spots_sets=['ALL', 'CAN_CANCEL']\n",
    "    with_wo_CB_options=['with_CB', 'wo_CB']\n",
    "    event_date_full_names=['cancellation_confirmed', 'cancellation_requested']\n",
    "\n",
    "#     all_spots_sets = []\n",
    "#     for spots_set in spots_sets:\n",
    "#         for with_wo_CB in with_wo_CB_options:\n",
    "#             for event_date_full_name in event_date_full_names:\n",
    "#                 if event_date_full_name == 'cancellation_requested':\n",
    "#                     event_date = 'canc_req'\n",
    "#                 elif event_date_full_name == 'cancellation_confirmed':\n",
    "#                     event_date = 'canc_conf'\n",
    "#                 all_spots_sets.append(spots_set+'_spots_'+with_wo_CB+'_'+event_date)\n",
    "\n",
    "    all_spots_sets = ['ALL_spots_with_CB_canc_req',\\\n",
    "                     'CAN_CANCEL_spots_wo_CB_canc_req']\n",
    "\n",
    "\n",
    "    cols_to_export = []\n",
    "    for set_ in all_spots_sets:\n",
    "        cols_to_export.append('%_of_'+set_)\n",
    "        cols_to_export.append('last_month_%_of_'+set_)\n",
    "\n",
    "        df = bucket.load_csv_from_s3(file_name = 'churn_analysis_based_on_behaviour/data/' + date_dir + \\\n",
    "                    '/exports/data_used_for_each_model/data_tv_'+set_+'.csv')\n",
    "        important_vars['type'] = important_vars['variable'].apply(lambda x: get_data_type(df, x))\n",
    "\n",
    "        important_vars['%_of_'+set_] = \\\n",
    "        important_vars[['variable', 'type']].apply(lambda x: get_perc_of_spots(df, x['variable'], x['type']), axis = 1)\n",
    "\n",
    "        important_vars['last_month_%_of_'+set_] = \\\n",
    "        important_vars[['variable', 'type']].apply(lambda x: get_perc_of_spots_last_month(df, x['variable'], x['type'], last_month), axis = 1)\n",
    "\n",
    "\n",
    "    all_spots = [#'ALL_spots_with_CB_cancellation_confirmed',\\\n",
    "                 'ALL_spots_with_CB_cancellation_requested']\n",
    "    can_cancel_spots = [#'CAN_CANCEL_spots_wo_CB_cancellation_confirmed',\\\n",
    "                       'CAN_CANCEL_spots_wo_CB_cancellation_requested']\n",
    "\n",
    "    for spots_set in all_spots+can_cancel_spots:\n",
    "#         important_vars[spots_set+'_exp(coef)_and_p_value'] = np.nan\n",
    "        important_vars[spots_set+'_exp(coef)_and_p_value'] = \\\n",
    "        important_vars[['variable_base_name', 'model', 'variable']].\\\n",
    "        apply(lambda x: get_coef_and_p_for_a_specific_model(var_base_name=x['variable_base_name'],\\\n",
    "                                                           var_=x['variable'],\\\n",
    "                                                           model=x['model'],\\\n",
    "                                                           spots_set=spots_set,\\\n",
    "                                                           model_names=model_names, date_dir=date_dir), axis = 1)\n",
    "        important_vars.loc[important_vars['interpretation'].isnull(), spots_set+'_exp(coef)_and_p_value'] = \\\n",
    "        important_vars.loc[important_vars['interpretation'].isnull(), ['variable_base_name', 'model', 'variable']].\\\n",
    "        apply(lambda x: get_coef_and_p_for_a_specific_model_for_non_behavioural_vars(var_base_name=x['variable_base_name'],\\\n",
    "                                                           var_=x['variable'],\\\n",
    "                                                           model=x['model'],\\\n",
    "                                                           spots_set=spots_set,\\\n",
    "                                                           model_names=model_names, date_dir=date_dir), axis = 1)\n",
    "\n",
    "    important_vars[['model', 'variable', 'interpretation', 'important_variables_group', 'exp(coef) - AVERAGE',\\\n",
    "                   'p value - AVERAGE']+[x+'_exp(coef)_and_p_value' for x in all_spots+can_cancel_spots]+cols_to_export].sort_values(['variable', 'interpretation']).\\\n",
    "    to_csv('data/'+date_dir+'/exports/important_variables/important_variables_sorted_by_variable_name.csv',\\\n",
    "          index = False)\n",
    "    bucket.store_csv_to_s3(data_frame = important_vars[['model', 'variable', 'interpretation', 'important_variables_group', 'exp(coef) - AVERAGE',\\\n",
    "                   'p value - AVERAGE']+[x+'_exp(coef)_and_p_value' for x in all_spots+can_cancel_spots]+cols_to_export].sort_values(['variable', 'interpretation']), \\\n",
    "            file_name = 'important_variables_sorted_by_variable_name.csv', \\\n",
    "            dir = '/churn_analysis_based_on_behaviour/data/'+date_dir+'/exports/important_variables/')\n",
    "    \n",
    "    return important_vars[['model', 'variable', 'interpretation', 'important_variables_group', 'exp(coef) - AVERAGE',\\\n",
    "                   'p value - AVERAGE']+[x+'_exp(coef)_and_p_value' for x in all_spots+can_cancel_spots]+cols_to_export].sort_values(['variable', 'interpretation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     model_params \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(file, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[0;32m      5\u001b[0m date_of_analysis \u001b[38;5;241m=\u001b[39m model_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_of_analysis\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m important_vars \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_of_analysis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_of_analysis\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(date_of_analysis)\u001b[0m\n\u001b[0;32m    228\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mdate_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/exports/important_variables/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    229\u001b[0m combined_export\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m    230\u001b[0m to_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mdate_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/exports/important_variables/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mfile_name, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 231\u001b[0m \u001b[43mbucket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore_csv_to_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_frame\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcombined_export\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/churn_analysis_based_on_behaviour/data/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdate_dir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/exports/important_variables/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_significant_variables_sorted_by_p_value.csv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    236\u001b[0m     sign_vars \u001b[38;5;241m=\u001b[39m combined_export\n",
      "File \u001b[1;32m~\\Projects_3_8\\churn\\helpers\\s3_bucket_utils.py:48\u001b[0m, in \u001b[0;36mS3BucketUtils.store_csv_to_s3\u001b[1;34m(self, data_frame, file_name, dir, index)\u001b[0m\n\u001b[0;32m     46\u001b[0m fs \u001b[38;5;241m=\u001b[39m s3fs\u001b[38;5;241m.\u001b[39mS3FileSystem(key\u001b[38;5;241m=\u001b[39msettings\u001b[38;5;241m.\u001b[39mS3_BUCKET_ACCESS_KEY, secret\u001b[38;5;241m=\u001b[39msettings\u001b[38;5;241m.\u001b[39mS3_BUCKET_SECRET_KEY)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(url\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m+\u001b[39mfile_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 48\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(bytes_to_write)\n",
      "File \u001b[1;32mc:\\users\\jovan\\projects_3_8\\env\\lib\\site-packages\\fsspec\\spec.py:1663\u001b[0m, in \u001b[0;36mAbstractBufferedFile.__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1662\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m-> 1663\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\jovan\\projects_3_8\\env\\lib\\site-packages\\fsspec\\spec.py:1630\u001b[0m, in \u001b[0;36mAbstractBufferedFile.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforced:\n\u001b[1;32m-> 1630\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39minvalidate_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n",
      "File \u001b[1;32mc:\\users\\jovan\\projects_3_8\\env\\lib\\site-packages\\fsspec\\spec.py:1501\u001b[0m, in \u001b[0;36mAbstractBufferedFile.flush\u001b[1;34m(self, force)\u001b[0m\n\u001b[0;32m   1498\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upload_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   1502\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n",
      "File \u001b[1;32mc:\\users\\jovan\\projects_3_8\\env\\lib\\site-packages\\s3fs\\core.py:2018\u001b[0m, in \u001b[0;36mS3File._upload_chunk\u001b[1;34m(self, final)\u001b[0m\n\u001b[0;32m   2015\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartNumber\u001b[39m\u001b[38;5;124m\"\u001b[39m: part, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m: out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[0;32m   2017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mand\u001b[39;00m final:\n\u001b[1;32m-> 2018\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m final\n",
      "File \u001b[1;32mc:\\users\\jovan\\projects_3_8\\env\\lib\\site-packages\\s3fs\\core.py:2033\u001b[0m, in \u001b[0;36mS3File.commit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2032\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m-> 2033\u001b[0m     write_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_s3\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2034\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mput_object\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mACL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2039\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2042\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\jovan\\projects_3_8\\env\\lib\\site-packages\\s3fs\\core.py:1885\u001b[0m, in \u001b[0;36mS3File._call_s3\u001b[1;34m(self, method, *kwarglist, **kwargs)\u001b[0m\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_s3\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;241m*\u001b[39mkwarglist, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwarglist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\jovan\\projects_3_8\\env\\lib\\site-packages\\fsspec\\asyn.py:85\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\jovan\\projects_3_8\\env\\lib\\site-packages\\fsspec\\asyn.py:53\u001b[0m, in \u001b[0;36msync\u001b[1;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open(r'./parameters/started_doing_something_report_parameters.yaml') as file:\n",
    "    model_params = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "date_of_analysis = model_params['date_of_analysis']\n",
    "important_vars = main(date_of_analysis=date_of_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_vars[important_vars['variable']=='had_properly_used_catering_inquiries_last_4_months']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_vars.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = 'had_properly_used_catering_inquiries_last_4_months'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ('last_') in 'had_properly_used_catering_inquiries_last_4_months':\n",
    "    print(bla.split('_last_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_and_p_values = \\\n",
    "        bucket.load_csv_from_s3(file_name='churn_analysis_based_on_behaviour/data/'+'2022_09_01'+'/exports/coefficients_and_pvalues/'+\\\n",
    "                               'started_doing_something/model_'+'1'+'/coef_and_pvalues_'+'CAN_CANCEL_spots_wo_CB_cancellation_requested'+'_p_below_0_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_and_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
